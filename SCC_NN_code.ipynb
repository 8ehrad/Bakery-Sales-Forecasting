{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all necesarry libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m  \n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math  \n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import r2_score\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive') # mount google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating functions to create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# functions\n",
    "# creating a function to create a new model\n",
    "# fit model on dataset\n",
    "def fit_model(train_feature, train_target, hidden_layer_size, epochs, validation_split = 0.2):\n",
    "  # define model\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Dense(units=hidden_layer_size,  # first dense layer\n",
    "                        kernel_initializer='lecun_normal',\n",
    "                        activation='selu'),\n",
    "    keras.layers.Dense(units=hidden_layer_size,  # second dense layer\n",
    "                        kernel_initializer='lecun_normal',\n",
    "                        activation='selu'),\n",
    "    keras.layers.Dropout(0.2), # dropout layer\n",
    "    keras.layers.Dense(1),  # output layer\n",
    "  ])\n",
    "  # optimiser for the model with loss function\n",
    "  model.compile(loss=tf.keras.losses.mae, \n",
    "              optimizer = tf.keras.optimizers.Adam(), \n",
    "              metrics=[\"mae\"])\n",
    "  # training the model\n",
    "  model.fit(train_feature,\n",
    "            train_target, \n",
    "            epochs = epochs,\n",
    "            validation_split = validation_split)\n",
    "  \n",
    "  return model\n",
    "\n",
    "# make an ensemble prediction for regression\n",
    "def ensemble_predictions(models, validation_feature):\n",
    "  # make predictions\n",
    "  yhats = [model.predict(validation_feature) for model in models]\n",
    "  yhats = np.array(yhats)\n",
    "  # find mean across ensemble members\n",
    "  result = np.mean(yhats, axis=0)\n",
    "  return result\n",
    "\n",
    "# evaluate ensemble model\n",
    "def evaluate_members(models, validation_feature, validation_target):\n",
    "  yhat = ensemble_predictions(models, validation_feature.to_numpy()) # make prediction\n",
    "  MSE = np.square(np.subtract(yhat, validation_target.to_numpy())).mean() # calculating MSE\n",
    "  r2 = r2_score(validation_target.to_numpy(), yhat) # finding pseudo R2\n",
    "  return math.sqrt(MSE); r2\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(start_layers, finish_layers, number_of_same_model):\n",
    "  all_models = list()\n",
    "  for i in range(start_layers, finish_layers + 1):\n",
    "    for j in range(number_of_same_model):\n",
    "      # define filename for this ensemble\n",
    "      filename = '/content/drive/MyDrive/Colab_Notebooks/Models_460/Models/model_' + str(j + 1) + '_hidden_layers_' + str(i) + '.h5'\n",
    "      # load model from file\n",
    "      model = load_model(filename)\n",
    "      model.trainable = False # freezing the model\n",
    "      model.compile(loss=tf.keras.losses.mae, \n",
    "              optimizer = tf.keras.optimizers.Adam(), \n",
    "              metrics=[\"mae\"]) # recompiling the model\n",
    "      # add to list of members\n",
    "      all_models.append(model) # adding the models to the list\n",
    "  return all_models\n",
    "\n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(models):\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in models] # creating a list of inputs for each model, should be 45 for all models\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in models] # creating a list of outputs for each model should be 1 for all models\n",
    "    merge = concatenate(ensemble_outputs) # concatenating all models\n",
    "    hidden_1 = Dense(100, activation='relu')(merge) # creating a dense layer of 100 neurons\n",
    "    Dropout_1 = Dropout(0.2)(hidden_1) # creating a dropout layer with 0.2 dropout rate\n",
    "    output = Dense(1)(Dropout_1) # creating a dense layer of 1 neuron for the output of a regression model\n",
    "    model = Model(inputs = ensemble_visible, outputs = output) #creating a model with the inputs and outputs\n",
    "    # compile\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error']) # compiling the model\n",
    "    return model\n",
    "  \n",
    "  # fit a stacked model\n",
    "def fit_stacked_model(model, feature_set, target_set):\n",
    "\t# prepare input data\n",
    "\tX = [feature_set for _ in range(len(model.input))]\n",
    "\t# training model\n",
    "\tmodel.fit(X, target_set, epochs = 20, validation_split = 0.2)\n",
    "\n",
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, feature_set, target_set):\n",
    "  # prepare input data\n",
    "  X = [feature_set for _ in range(len(model.input))]\n",
    "  # make prediction\n",
    "  yhat = model.predict(X, verbose=0)\n",
    "  mse = np.square(np.subtract(yhat, target_set.to_numpy())).mean() # calculating MSE\n",
    "  r2 = r2_score(target_set.to_numpy(), yhat) # finding pseudo R2\n",
    "  return math.sqrt(mse), r2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating training set and validation\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Models_460/Cleaned_Data_Python.csv') # Read the data\n",
    "\n",
    "# Convert the date to datetime64\n",
    "df['Transaction_date'] = pd.to_datetime(df['Transaction_date'], format='%Y-%m-%d')\n",
    "\n",
    "validation_df = df.loc[(df['Transaction_date'] >= '2022-08-24')] # validation set\n",
    "validation_df.drop('Transaction_date', axis = 1, inplace=True) # validation set with no dates\n",
    "rest_df = df.loc[(df['Transaction_date'] < '2022-08-24')] # traing set\n",
    "rest_df.drop('Transaction_date', axis = 1, inplace=True) # training set with no dates\n",
    "\n",
    "\n",
    "target_validation = validation_df['Full_Price_Sales_Quantity'] # target validation\n",
    "feature_validation = validation_df.drop('Full_Price_Sales_Quantity', axis = 1)  # feature validation\n",
    "target_training = rest_df['Full_Price_Sales_Quantity'] # target training\n",
    "features_training = rest_df.drop('Full_Price_Sales_Quantity', axis = 1) # feature training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_layers = 25 # node size to start with\n",
    "finish_layers = 45 # node size to finish with 45\n",
    "number_of_same_model = 5 # number of same model to run\n",
    "epochs = 10 # number of epochs\n",
    "for i in range(start_layers, finish_layers + 1):\n",
    "  for j in range(number_of_same_model):\n",
    "  # training the model\n",
    "    model = fit_model(features_training, target_training, hidden_layer_size = i, epochs = epochs)\n",
    "    # creating a filename for the model\n",
    "    filename = '/content/drive/MyDrive/Colab_Notebooks/Models_460/Models/model_' + str(j + 1) + '_hidden_layers_' + str(i) + '.h5' \n",
    "    # saving trained model to be used later\n",
    "    model.save(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating prediction for validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all models\n",
    "start_layers = 25 # node size to start with\n",
    "finish_layers = 45 # node size to finish with 45\n",
    "number_of_same_model = 5 # number of same model to run\n",
    "models = load_all_models(start_layers, finish_layers, number_of_same_model) # loading all models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a new model by combaning all the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(models)\n",
    "# training the emsemble model\n",
    "fit_stacked_model(stacked_model, features_training, target_training)\n",
    "stacked_model.save('/content/drive/MyDrive/Colab_Notebooks/Models_460/Models/best_model.h5') # saving trained model to be used later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading ensemble model\n",
    "best_file_name = '/content/drive/MyDrive/Colab_Notebooks/Models_460/Models/best_model.h5' # file name of the best model\n",
    "best_model = load_model(best_file_name) # lodading the best model\n",
    "best_model.compile(loss='mae', optimizer='adam', metrics=['mae']) # compiling the best model\n",
    "# plot_model(stacked_model, show_shapes=True, to_file='model_graph.png') # plotting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# make predictions and evaluate\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# loading model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m best_file_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/content/drive/MyDrive/Colab_Notebooks/Models_460/Models/best_model.h5\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m best_model \u001b[39m=\u001b[39m load_model(best_file_name)\n\u001b[0;32m      5\u001b[0m best_model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[39m# plot_model(stacked_model, show_shapes=True, to_file='model_graph.png')\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[39m#training\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "# make predictions and evaluate\n",
    "\n",
    "#training. This section requires a lot of ram so it might be good to comment it out\n",
    "rmse_value_val, r2_val = predict_stacked_model(stacked_model, features_training, target_training)\n",
    "print(rmse_value_val, r2_val)\n",
    "rmse_value_val, r2_val = predict_stacked_model(best_model, features_training, target_training)\n",
    "print(rmse_value_val, r2_val)\n",
    "\n",
    "# validation\n",
    "rmse_value_val, r2_val = predict_stacked_model(stacked_model, feature_validation, target_validation)\n",
    "print(rmse_value_val, r2_val)\n",
    "rmse_value_val, r2_val = predict_stacked_model(best_model, feature_validation, target_validation)\n",
    "print(rmse_value_val, r2_val)\n",
    "\n",
    "# average of the models instead of concatenating all of them and then training the ensemble model\n",
    "# # models = [fit_model(features_training, target_training, hidden_layer_size = i, epochs = epochs) for j in range(start_layers, finish_layers + 1) for i in range(number_of_same_model + 1)]\n",
    "# RMSE_value = evaluate_members(models, feature_validation, target_validation)\n",
    "# print(RMSE_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "25484fbb9aa42e77b39fbe8a0d86f1f796036f094b2c5f4e76075e10f77cea3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
